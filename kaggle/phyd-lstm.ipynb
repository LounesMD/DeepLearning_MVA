{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10320827,"sourceType":"datasetVersion","datasetId":6389997}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport numpy as np\nimport os \nfrom PIL import Image\nfrom collections import defaultdict\nfrom torch.utils.data import Dataset\n\n\ndef get_images_from_path(folder_path):\n    # Dictionary to hold images, with subfolders as keys and images sorted by index\n    images_by_subfolder = defaultdict(list)\n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            if file.endswith('.png') and 'img_' in file:\n                try:\n                    # Extract the numeric index x from the filename \"img_x.png\"\n                    index = int(file.split('_')[1].split('.')[0])  # Extract x from \"img_x.png\"\n                    # Get the relative subfolder path (excluding the base folder path)\n                    subfolder_name = os.path.basename(root)\n                    img_path = os.path.join(root, file)\n                    # Use context manager to ensure the file is properly closed\n                    with Image.open(img_path) as img:\n                        # Append the image to the appropriate subfolder's list, sorted by index\n                        images_by_subfolder[subfolder_name].append((index, img.convert('L')))\n                except (ValueError, IndexError):\n                    print(f\"Skipping file with unexpected format: {file}\")\n    # Sort images within each subfolder by their index\n    for subfolder, images in images_by_subfolder.items():\n        # Sort by the numeric index (first element of the tuple)\n        images_by_subfolder[subfolder] = [img for _, img in sorted(images, key=lambda x: x[0])]\n\n\n    # Assuming images_by_subfolder is already populated\n    # Create an empty list to store image data in the desired tensor shape\n    tensor_images = []\n    # Define the target image size (64x64)\n    target_size = (64, 64)\n    # Iterate through each subfolder\n    for subfolder, images in images_by_subfolder.items():\n        folder_images = []\n        \n        # Iterate through each image in the subfolder\n        for img in images:\n            # Resize the image to (64, 64)\n            img_resized = img.resize(target_size)\n            \n            # Convert the image to a NumPy array and normalize to [0, 1] range\n            img_array = np.array(img_resized) / 255.0  # Convert to float and normalize\n            \n            # Add a channel dimension (grayscale, so it's 1 channel)\n            img_array = np.expand_dims(img_array, axis=-1)  # Shape becomes (64, 64, 1)\n            \n            # Add the image to the list\n            folder_images.append(img_array)\n        \n        # Append the images from the current subfolder to the main list\n        tensor_images.append(folder_images)\n    # Convert the list to a NumPy array and then to a PyTorch tensor\n    # Convert the list to a numpy array of shape (len(subfolders), num_images_per_subfolder, 64, 64, 1)\n    tensor_images = np.array(tensor_images)\n    # Convert to a PyTorch tensor (shape will be (batch_size, num_images, 1, 64, 64))\n    tensor_images = torch.tensor(tensor_images)\n    tensor_images = tensor_images.permute(0, 1, 4, 2, 3)\n    return tensor_images\n\n\nclass ImageDataset(Dataset):\n    def __init__(self, tensor_images):\n        \"\"\"\n        Args:\n            tensor_images (Tensor): A tensor of shape (num_subfolders, 15, 1, 64, 64)\n        \"\"\"\n        self.tensor_images = tensor_images\n        self.num_subfolders = tensor_images.shape[0]\n        self.num_images_per_subfolder = tensor_images.shape[1]\n\n    def __len__(self):\n        # Return the number of subfolders\n        return self.num_subfolders\n\n    def __getitem__(self, idx):\n        # Get the subfolder images\n        subfolder_images = self.tensor_images[idx]\n\n        # First element: None\n        first_element = 0\n\n        # Second element: First 10 images (index 0 to 9)\n        second_element = subfolder_images[:10]\n\n        # Third element: Last 5 images (index 10 to 14)\n        third_element = subfolder_images[10:]\n\n        # Return as a tuple\n        return first_element, second_element, third_element\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T15:45:51.456015Z","iopub.execute_input":"2024-12-29T15:45:51.456322Z","iopub.status.idle":"2024-12-29T15:45:55.164740Z","shell.execute_reply.started":"2024-12-29T15:45:51.456291Z","shell.execute_reply":"2024-12-29T15:45:55.163871Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Path to the folder containing images\ntrain_dataset = get_images_from_path('/kaggle/input/weather-data/weather_dataset/train')\ntrain_dataset = ImageDataset(train_dataset)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T15:45:55.165961Z","iopub.execute_input":"2024-12-29T15:45:55.166397Z","iopub.status.idle":"2024-12-29T16:00:52.310215Z","shell.execute_reply.started":"2024-12-29T15:45:55.166363Z","shell.execute_reply":"2024-12-29T16:00:52.309536Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"test_dataset = get_images_from_path('/kaggle/input/weather-data/weather_dataset/test')\ntest_dataset = ImageDataset(test_dataset)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\nval_dataset = get_images_from_path('/kaggle/input/weather-data/weather_dataset/validation')\nval_dataset = ImageDataset(val_dataset)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T16:00:52.311428Z","iopub.execute_input":"2024-12-29T16:00:52.311755Z","iopub.status.idle":"2024-12-29T16:12:10.060148Z","shell.execute_reply.started":"2024-12-29T16:00:52.311731Z","shell.execute_reply":"2024-12-29T16:12:10.059227Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"##### ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport numpy as np\nimport random\nimport time\nfrom tqdm import tqdm\nfrom skimage.metrics import structural_similarity as ssim\nimport matplotlib.pyplot as plt\nimport torch.utils.data as data\n\nimport argparse\nimport os\nimport gzip\nimport numpy as np\n\n##############################################################################################################\nfrom numpy import *\nfrom numpy.linalg import *\nfrom scipy.special import factorial\nfrom functools import reduce\nimport torch\nimport torch.nn as nn\n\nfrom functools import reduce\n\n__all__ = ['M2K','K2M']\n\ndef _apply_axis_left_dot(x, mats):\n    assert x.dim() == len(mats)+1\n    sizex = x.size()\n    k = x.dim()-1\n    for i in range(k):\n        x = tensordot(mats[k-i-1], x, dim=[1,k])\n    x = x.permute([k,]+list(range(k))).contiguous()\n    x = x.view(sizex)\n    return x\n\ndef _apply_axis_right_dot(x, mats):\n    assert x.dim() == len(mats)+1\n    sizex = x.size()\n    k = x.dim()-1\n    x = x.permute(list(range(1,k+1))+[0,])\n    for i in range(k):\n        x = tensordot(x, mats[i], dim=[0,0])\n    x = x.contiguous()\n    x = x.view(sizex)\n    return x\n\nclass _MK(nn.Module):\n    def __init__(self, shape):\n        super(_MK, self).__init__()\n        self._size = torch.Size(shape)\n        self._dim = len(shape)\n        M = []\n        invM = []\n        assert len(shape) > 0\n        j = 0\n        for l in shape:\n            M.append(zeros((l,l)))\n            for i in range(l):\n                M[-1][i] = ((arange(l)-(l-1)//2)**i)/factorial(i)\n            invM.append(inv(M[-1]))\n            self.register_buffer('_M'+str(j), torch.from_numpy(M[-1]))\n            self.register_buffer('_invM'+str(j), torch.from_numpy(invM[-1]))\n            j += 1\n\n    @property\n    def M(self):\n        return list(self._buffers['_M'+str(j)] for j in range(self.dim()))\n    @property\n    def invM(self):\n        return list(self._buffers['_invM'+str(j)] for j in range(self.dim()))\n\n    def size(self):\n        return self._size\n    def dim(self):\n        return self._dim\n    def _packdim(self, x):\n        assert x.dim() >= self.dim()\n        if x.dim() == self.dim():\n            x = x[newaxis,:]\n        x = x.contiguous()\n        x = x.view([-1,]+list(x.size()[-self.dim():]))\n        return x\n\n    def forward(self):\n        pass\n\nclass M2K(_MK):\n    \"\"\"\n    convert moment matrix to convolution kernel\n    Arguments:\n        shape (tuple of int): kernel shape\n    Usage:\n        m2k = M2K([5,5])\n        m = torch.randn(5,5,dtype=torch.float64)\n        k = m2k(m)\n    \"\"\"\n    def __init__(self, shape):\n        super(M2K, self).__init__(shape)\n    def forward(self, m):\n        \"\"\"\n        m (Tensor): torch.size=[...,*self.shape]\n        \"\"\"\n        sizem = m.size()\n        m = self._packdim(m)\n        m = _apply_axis_left_dot(m, self.invM)\n        m = m.view(sizem)\n        return m\n\nclass K2M(_MK):\n    \"\"\"\n    convert convolution kernel to moment matrix\n    Arguments:\n        shape (tuple of int): kernel shape\n    Usage:\n        k2m = K2M([5,5])\n        k = torch.randn(5,5,dtype=torch.float64)\n        m = k2m(k)\n    \"\"\"\n    def __init__(self, shape):\n        super(K2M, self).__init__(shape)\n    def forward(self, k):\n        \"\"\"\n        k (Tensor): torch.size=[...,*self.shape]\n        \"\"\"\n        sizek = k.size()\n        k = self._packdim(k)\n        k = _apply_axis_left_dot(k, self.M)\n        k = k.view(sizek)\n        return k\n\n\n    \ndef tensordot(a,b,dim):\n    \"\"\"\n    tensordot in PyTorch, see numpy.tensordot?\n    \"\"\"\n    l = lambda x,y:x*y\n    if isinstance(dim,int):\n        a = a.contiguous()\n        b = b.contiguous()\n        sizea = a.size()\n        sizeb = b.size()\n        sizea0 = sizea[:-dim]\n        sizea1 = sizea[-dim:]\n        sizeb0 = sizeb[:dim]\n        sizeb1 = sizeb[dim:]\n        N = reduce(l, sizea1, 1)\n        assert reduce(l, sizeb0, 1) == N\n    else:\n        adims = dim[0]\n        bdims = dim[1]\n        adims = [adims,] if isinstance(adims, int) else adims\n        bdims = [bdims,] if isinstance(bdims, int) else bdims\n        adims_ = set(range(a.dim())).difference(set(adims))\n        adims_ = list(adims_)\n        adims_.sort()\n        perma = adims_+adims\n        bdims_ = set(range(b.dim())).difference(set(bdims))\n        bdims_ = list(bdims_)\n        bdims_.sort()\n        permb = bdims+bdims_\n        a = a.permute(*perma).contiguous()\n        b = b.permute(*permb).contiguous()\n\n        sizea = a.size()\n        sizeb = b.size()\n        sizea0 = sizea[:-len(adims)]\n        sizea1 = sizea[-len(adims):]\n        sizeb0 = sizeb[:len(bdims)]\n        sizeb1 = sizeb[len(bdims):]\n        N = reduce(l, sizea1, 1)\n        assert reduce(l, sizeb0, 1) == N\n    a = a.view([-1,N])\n    b = b.view([N,-1])\n    c = a@b\n    return c.view(sizea0+sizeb1)\n\n##############################################################################################################\nimport torch\nimport torch.nn as nn\n\nclass PhyCell_Cell(nn.Module):\n    def __init__(self, input_dim, F_hidden_dim, kernel_size, bias=1):\n        super(PhyCell_Cell, self).__init__()\n        self.input_dim  = input_dim\n        self.F_hidden_dim = F_hidden_dim\n        self.kernel_size = kernel_size\n        self.padding     = kernel_size[0] // 2, kernel_size[1] // 2\n        self.bias = bias\n        \n        self.F = nn.Sequential()\n        self.F.add_module('conv1', nn.Conv2d(in_channels=input_dim, out_channels=F_hidden_dim, kernel_size=self.kernel_size, stride=(1,1), padding=self.padding))\n        self.F.add_module('bn1',nn.GroupNorm( 7 ,F_hidden_dim))        \n        self.F.add_module('conv2', nn.Conv2d(in_channels=F_hidden_dim, out_channels=input_dim, kernel_size=(1,1), stride=(1,1), padding=(0,0)))\n\n        self.convgate = nn.Conv2d(in_channels=self.input_dim + self.input_dim,\n                              out_channels= self.input_dim,\n                              kernel_size=(3,3),\n                              padding=(1,1), bias=self.bias)\n\n    def forward(self, x, hidden): # x [batch_size, hidden_dim, height, width]      \n        combined = torch.cat([x, hidden], dim=1)  # concatenate along channel axis\n        combined_conv = self.convgate(combined)\n        K = torch.sigmoid(combined_conv)\n        hidden_tilde = hidden + self.F(hidden)        # prediction\n        next_hidden = hidden_tilde + K * (x-hidden_tilde)   # correction , Haddamard product     \n        return next_hidden\n\nclass PhyCell(nn.Module):\n    def __init__(self, input_shape, input_dim, F_hidden_dims, n_layers, kernel_size, device):\n        super(PhyCell, self).__init__()\n        self.input_shape = input_shape\n        self.input_dim = input_dim\n        self.F_hidden_dims = F_hidden_dims\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.H = []  \n        self.device = device\n             \n        cell_list = []\n        for i in range(0, self.n_layers):\n            cell_list.append(PhyCell_Cell(input_dim=input_dim,\n                                          F_hidden_dim=self.F_hidden_dims[i],\n                                          kernel_size=self.kernel_size))                                     \n        self.cell_list = nn.ModuleList(cell_list)\n        \n       \n    def forward(self, input_, first_timestep=False): # input_ [batch_size, 1, channels, width, height]    \n        batch_size = input_.data.size()[0]\n        if (first_timestep):   \n            self.initHidden(batch_size) # init Hidden at each forward start\n              \n        for j,cell in enumerate(self.cell_list):\n            if j==0: # bottom layer\n                self.H[j] = cell(input_, self.H[j])\n            else:\n                self.H[j] = cell(self.H[j-1],self.H[j])\n        \n        return self.H , self.H \n    \n    def initHidden(self,batch_size):\n        self.H = [] \n        for i in range(self.n_layers):\n            self.H.append( torch.zeros(batch_size, self.input_dim, self.input_shape[0], self.input_shape[1]).to(self.device) )\n\n    def setHidden(self, H):\n        self.H = H\n\n        \nclass ConvLSTM_Cell(nn.Module):\n    def __init__(self, input_shape, input_dim, hidden_dim, kernel_size, bias=1):              \n        \"\"\"\n        input_shape: (int, int)\n            Height and width of input tensor as (height, width).\n        input_dim: int\n            Number of channels of input tensor.\n        hidden_dim: int\n            Number of channels of hidden state.\n        kernel_size: (int, int)\n            Size of the convolutional kernel.\n        bias: bool\n            Whether or not to add the bias.\n        \"\"\"\n        super(ConvLSTM_Cell, self).__init__()\n        \n        self.height, self.width = input_shape\n        self.input_dim  = input_dim\n        self.hidden_dim = hidden_dim\n        self.kernel_size = kernel_size\n        self.padding     = kernel_size[0] // 2, kernel_size[1] // 2\n        self.bias        = bias\n        \n        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n                              out_channels=4 * self.hidden_dim,\n                              kernel_size=self.kernel_size,\n                              padding=self.padding, bias=self.bias)\n                 \n    # we implement LSTM that process only one timestep \n    def forward(self,x, hidden): # x [batch, hidden_dim, width, height]          \n        h_cur, c_cur = hidden\n        \n        combined = torch.cat([x, h_cur], dim=1)  # concatenate along channel axis\n        combined_conv = self.conv(combined)\n        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1) \n        i = torch.sigmoid(cc_i)\n        f = torch.sigmoid(cc_f)\n        o = torch.sigmoid(cc_o)\n        g = torch.tanh(cc_g)\n\n        c_next = f * c_cur + i * g\n        h_next = o * torch.tanh(c_next)\n        return h_next, c_next\n\n\nclass ConvLSTM(nn.Module):\n    def __init__(self, input_shape, input_dim, hidden_dims, n_layers, kernel_size,device):\n        super(ConvLSTM, self).__init__()\n        self.input_shape = input_shape\n        self.input_dim = input_dim\n        self.hidden_dims = hidden_dims\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.H, self.C = [],[]   \n        self.device = device\n        \n        cell_list = []\n        for i in range(0, self.n_layers):\n            cur_input_dim = self.input_dim if i == 0 else self.hidden_dims[i-1]\n            print('layer ',i,'input dim ', cur_input_dim, ' hidden dim ', self.hidden_dims[i])\n            cell_list.append(ConvLSTM_Cell(input_shape=self.input_shape,\n                                          input_dim=cur_input_dim,\n                                          hidden_dim=self.hidden_dims[i],\n                                          kernel_size=self.kernel_size))                                     \n        self.cell_list = nn.ModuleList(cell_list)\n        \n       \n    def forward(self, input_, first_timestep=False): # input_ [batch_size, 1, channels, width, height]    \n        batch_size = input_.data.size()[0]\n        if (first_timestep):   \n            self.initHidden(batch_size) # init Hidden at each forward start\n              \n        for j,cell in enumerate(self.cell_list):\n            if j==0: # bottom layer\n                self.H[j], self.C[j] = cell(input_, (self.H[j],self.C[j]))\n            else:\n                self.H[j], self.C[j] = cell(self.H[j-1],(self.H[j],self.C[j]))\n        \n        return (self.H,self.C) , self.H   # (hidden, output)\n    \n    def initHidden(self,batch_size):\n        self.H, self.C = [],[]  \n        for i in range(self.n_layers):\n            self.H.append( torch.zeros(batch_size,self.hidden_dims[i], self.input_shape[0], self.input_shape[1]).to(self.device) )\n            self.C.append( torch.zeros(batch_size,self.hidden_dims[i], self.input_shape[0], self.input_shape[1]).to(self.device) )\n    \n    def setHidden(self, hidden):\n        H,C = hidden\n        self.H, self.C = H,C\n \n\nclass dcgan_conv(nn.Module):\n    def __init__(self, nin, nout, stride):\n        super(dcgan_conv, self).__init__()\n        self.main = nn.Sequential(\n                nn.Conv2d(in_channels=nin, out_channels=nout, kernel_size=(3,3), stride=stride, padding=1),\n                nn.GroupNorm(16,nout),\n                nn.LeakyReLU(0.2, inplace=True),\n                )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass dcgan_upconv(nn.Module):\n    def __init__(self, nin, nout, stride):\n        super(dcgan_upconv, self).__init__()\n        if (stride ==2):\n            output_padding = 1\n        else:\n            output_padding = 0\n        self.main = nn.Sequential(\n                nn.ConvTranspose2d(in_channels=nin,out_channels=nout,kernel_size=(3,3), stride=stride,padding=1,output_padding=output_padding),\n                nn.GroupNorm(16,nout),\n                nn.LeakyReLU(0.2, inplace=True),\n                )\n\n    def forward(self, input):\n        return self.main(input)\n        \nclass encoder_E(nn.Module):\n    def __init__(self, nc=1, nf=32):\n        super(encoder_E, self).__init__()\n        # input is (1) x 64 x 64\n        self.c1 = dcgan_conv(nc, nf, stride=2) # (32) x 32 x 32\n        self.c2 = dcgan_conv(nf, nf, stride=1) # (32) x 32 x 32\n        self.c3 = dcgan_conv(nf, 2*nf, stride=2) # (64) x 16 x 16\n\n    def forward(self, input):\n        h1 = self.c1(input)  \n        h2 = self.c2(h1)    \n        h3 = self.c3(h2)\n        return h3\n\nclass decoder_D(nn.Module):\n    def __init__(self, nc=1, nf=32):\n        super(decoder_D, self).__init__()\n        self.upc1 = dcgan_upconv(2*nf, nf, stride=2) #(32) x 32 x 32\n        self.upc2 = dcgan_upconv(nf, nf, stride=1) #(32) x 32 x 32\n        self.upc3 = nn.ConvTranspose2d(in_channels=nf,out_channels=nc,kernel_size=(3,3),stride=2,padding=1,output_padding=1)  #(nc) x 64 x 64\n\n    def forward(self, input):      \n        d1 = self.upc1(input) \n        d2 = self.upc2(d1)\n        d3 = self.upc3(d2)  \n        return d3  \n\n\nclass encoder_specific(nn.Module):\n    def __init__(self, nc=64, nf=64):\n        super(encoder_specific, self).__init__()\n        self.c1 = dcgan_conv(nc, nf, stride=1) # (64) x 16 x 16\n        self.c2 = dcgan_conv(nf, nf, stride=1) # (64) x 16 x 16\n\n    def forward(self, input):\n        h1 = self.c1(input)  \n        h2 = self.c2(h1)     \n        return h2\n\nclass decoder_specific(nn.Module):\n    def __init__(self, nc=64, nf=64):\n        super(decoder_specific, self).__init__()\n        self.upc1 = dcgan_upconv(nf, nf, stride=1) #(64) x 16 x 16\n        self.upc2 = dcgan_upconv(nf, nc, stride=1) #(32) x 32 x 32\n        \n    def forward(self, input):\n        d1 = self.upc1(input) \n        d2 = self.upc2(d1)  \n        return d2       \n\n        \nclass EncoderRNN(torch.nn.Module):\n    def __init__(self,phycell,convcell, device):\n        super(EncoderRNN, self).__init__()\n        self.encoder_E = encoder_E()   # general encoder 64x64x1 -> 32x32x32\n        self.encoder_Ep = encoder_specific() # specific image encoder 32x32x32 -> 16x16x64\n        self.encoder_Er = encoder_specific() \n        self.decoder_Dp = decoder_specific() # specific image decoder 16x16x64 -> 32x32x32 \n        self.decoder_Dr = decoder_specific()     \n        self.decoder_D = decoder_D()  # general decoder 32x32x32 -> 64x64x1 \n\n        self.encoder_E = self.encoder_E.to(device)\n        self.encoder_Ep = self.encoder_Ep.to(device) \n        self.encoder_Er = self.encoder_Er.to(device) \n        self.decoder_Dp = self.decoder_Dp.to(device) \n        self.decoder_Dr = self.decoder_Dr.to(device)               \n        self.decoder_D = self.decoder_D.to(device)\n        self.phycell = phycell.to(device)\n        self.convcell = convcell.to(device)\n\n    def forward(self, input, first_timestep=False, decoding=False):\n        input = self.encoder_E(input) # general encoder 64x64x1 -> 32x32x32\n    \n        if decoding:  # input=None in decoding phase\n            input_phys = None\n        else:\n            input_phys = self.encoder_Ep(input)\n        input_conv = self.encoder_Er(input)     \n\n        hidden1, output1 = self.phycell(input_phys, first_timestep)\n        hidden2, output2 = self.convcell(input_conv, first_timestep)\n\n        decoded_Dp = self.decoder_Dp(output1[-1])\n        decoded_Dr = self.decoder_Dr(output2[-1])\n        \n        out_phys = torch.sigmoid(self.decoder_D(decoded_Dp)) # partial reconstructions for vizualization\n        out_conv = torch.sigmoid(self.decoder_D(decoded_Dr))\n\n        concat = decoded_Dp + decoded_Dr   \n        output_image = torch.sigmoid( self.decoder_D(concat ))\n        return out_phys, hidden1, output_image, out_phys, out_conv        \n\n##############################################################################################################\n\ndevice = torch.device(\"cuda\")\n\nconstraints = torch.zeros((49,7,7)).to(device)\nind = 0\nfor i in range(0,7):\n    for j in range(0,7):\n        constraints[ind,i,j] = 1\n        ind +=1    \n\ndef train_on_batch(input_tensor, target_tensor, encoder, encoder_optimizer, criterion,teacher_forcing_ratio):                \n    encoder_optimizer.zero_grad()\n    # input_tensor : torch.Size([batch_size, input_length, channels, cols, rows])\n    input_length  = input_tensor.size(1)\n    target_length = target_tensor.size(1)\n    loss = 0\n    for ei in range(input_length-1): \n        encoder_output, encoder_hidden, output_image,_,_ = encoder(input_tensor[:,ei,:,:,:].float(), (ei==0) )\n        loss += criterion(output_image,input_tensor[:,ei+1,:,:,:])\n\n    decoder_input = input_tensor[:,-1,:,:,:] # first decoder input = last image of input sequence\n    \n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False \n    for di in range(target_length):\n        decoder_output, decoder_hidden, output_image,_,_ = encoder(decoder_input.float())\n        target = target_tensor[:,di,:,:,:]\n        loss += criterion(output_image,target)\n        if use_teacher_forcing:\n            decoder_input = target # Teacher forcing    \n        else:\n            decoder_input = output_image\n\n    # Moment regularization  # encoder.phycell.cell_list[0].F.conv1.weight # size (nb_filters,in_channels,7,7)\n    k2m = K2M([7,7]).to(device)\n    for b in range(0,encoder.phycell.cell_list[0].input_dim):\n        filters = encoder.phycell.cell_list[0].F.conv1.weight[:,b,:,:] # (nb_filters,7,7)     \n        m = k2m(filters.double()) \n        m  = m.float()   \n        loss += criterion(m, constraints) # constrains is a precomputed matrix   \n    loss.backward()\n    encoder_optimizer.step()\n    return loss.item() / target_length\n\n\ndef trainIters(encoder, nepochs, print_every=10,eval_every=10,name=''):\n    train_losses = []\n    best_mse = float('inf')\n\n    encoder_optimizer = torch.optim.Adam(encoder.parameters(),lr=0.001)\n    scheduler_enc = ReduceLROnPlateau(encoder_optimizer, mode='min', patience=2,factor=0.1,verbose=True)\n    criterion = nn.MSELoss()\n    \n    for epoch in tqdm(range(0, nepochs)):        \n        t0 = time.time()\n        loss_epoch = 0\n        teacher_forcing_ratio = np.maximum(0 , 1 - epoch * 0.003) \n        \n        for i, out in enumerate(train_loader, 0):\n            input_tensor = out[1].to(device)\n            target_tensor = out[2].to(device)\n            loss = train_on_batch(input_tensor.float(), target_tensor.float(), encoder, encoder_optimizer, criterion, teacher_forcing_ratio)                                   \n            loss_epoch += loss\n                      \n        train_losses.append(loss_epoch)        \n        if (epoch+1) % print_every == 0:\n            print('epoch ',epoch,  ' loss ',loss_epoch, ' time epoch ',time.time()-t0)\n            \n        if (epoch+1) % eval_every == 0:\n            mse, mae,ssim = evaluate(encoder,test_loader) \n            scheduler_enc.step(mse)                   \n            torch.save(encoder.state_dict(),'encoder_{}.pth'.format(name))                           \n    return train_losses\n\n    \ndef evaluate(encoder,loader):\n    total_mse, total_mae,total_ssim,total_bce = 0,0,0,0\n    t0 = time.time()\n    with torch.no_grad():\n        for i, out in enumerate(loader, 0):\n            input_tensor = out[1].to(device)\n            target_tensor = out[2].to(device)\n            input_length = input_tensor.size()[1]\n            target_length = target_tensor.size()[1]\n\n            for ei in range(input_length-1):\n                encoder_output, encoder_hidden, _,_,_  = encoder(input_tensor[:,ei,:,:,:].float(), (ei==0))\n\n            decoder_input = input_tensor[:,-1,:,:,:] # first decoder input= last image of input sequence\n            predictions = []\n\n            for di in range(target_length):\n                decoder_output, decoder_hidden, output_image,_,_ = encoder(decoder_input.float(), False, False)\n                decoder_input = output_image\n                predictions.append(output_image.cpu())\n\n            input = input_tensor.cpu().numpy()\n            target = target_tensor.cpu().numpy()\n            predictions =  np.stack(predictions) # (10, batch_size, 1, 64, 64)\n            predictions = predictions.swapaxes(0,1)  # (batch_size,10, 1, 64, 64)\n\n            mse_batch = np.mean((predictions-target)**2 , axis=(0,1,2)).sum()\n            mae_batch = np.mean(np.abs(predictions-target) ,  axis=(0,1,2)).sum() \n            total_mse += mse_batch\n            total_mae += mae_batch\n            for a in range(0,target.shape[0]):\n                for b in range(0,target.shape[1]):\n                    total_ssim += ssim(target[a,b,0,], predictions[a,b,0,], data_range=1.0) / (target.shape[0]*target.shape[1]) \n\n            \n            cross_entropy = -target*np.log(predictions) - (1-target) * np.log(1-predictions)\n            cross_entropy = cross_entropy.sum()\n            cross_entropy = cross_entropy / (64*target_length)\n            total_bce +=  cross_entropy\n     \n    print('eval mse ', total_mse/len(loader),  ' eval mae ', total_mae/len(loader),' eval ssim ',total_ssim/len(loader), ' time= ', time.time()-t0)        \n    return total_mse/len(loader),  total_mae/len(loader), total_ssim/len(loader)\n\n\nphycell  =  PhyCell(input_shape=(16,16), input_dim=64, F_hidden_dims=[49], n_layers=1, kernel_size=(7,7), device=device) \nconvcell =  ConvLSTM(input_shape=(16,16), input_dim=64, hidden_dims=[128,128,64], n_layers=3, kernel_size=(3,3), device=device)   \nencoder  = EncoderRNN(phycell, convcell, device)\n  \ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n   \nprint('phycell ' , count_parameters(phycell) )    \nprint('convcell ' , count_parameters(convcell) ) \nprint('encoder ' , count_parameters(encoder) ) \n\ntrainIters(encoder,200,print_every=1,eval_every=10,name=\"phydnet_eval\")\n\n#encoder.load_state_dict(torch.load('save/encoder_phydnet.pth'))\n#encoder.eval()\n#mse, mae,ssim = evaluate(encoder,test_loader) \ntorch.save(encoder.state_dict(),'encoder_{}.pth'.format('phydnet_weather_200'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T16:12:10.061360Z","iopub.execute_input":"2024-12-29T16:12:10.061683Z","iopub.status.idle":"2024-12-29T20:55:30.418671Z","shell.execute_reply.started":"2024-12-29T16:12:10.061653Z","shell.execute_reply":"2024-12-29T20:55:30.417760Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-4-6c2fca578c90>:591: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"layer  0 input dim  64  hidden dim  128\nlayer  1 input dim  128  hidden dim  128\nlayer  2 input dim  128  hidden dim  64\nphycell  230803\nconvcell  2508032\nencoder  3091732\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 1/200 [01:23<4:38:24, 83.94s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  0  loss  29.871303391456607  time epoch  83.94046664237976\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 2/200 [02:47<4:35:26, 83.47s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  1  loss  18.21758080720901  time epoch  83.13774037361145\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 3/200 [04:10<4:33:15, 83.22s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  2  loss  16.350564229488374  time epoch  82.93031549453735\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 4/200 [05:33<4:31:44, 83.19s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  3  loss  14.98818594217301  time epoch  83.12866950035095\n","output_type":"stream"},{"name":"stderr","text":"  2%|▎         | 5/200 [06:56<4:30:01, 83.09s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  4  loss  14.152812612056737  time epoch  82.90316200256348\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 6/200 [08:19<4:28:35, 83.07s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  5  loss  13.630245256423947  time epoch  83.03374648094177\n","output_type":"stream"},{"name":"stderr","text":"  4%|▎         | 7/200 [09:42<4:27:08, 83.05s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  6  loss  13.092449259757991  time epoch  83.0111517906189\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 8/200 [11:05<4:25:48, 83.07s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  7  loss  12.690856075286861  time epoch  83.10028100013733\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 9/200 [12:28<4:24:23, 83.05s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  8  loss  12.244361352920532  time epoch  83.02533769607544\nepoch  9  loss  11.824924880266186  time epoch  83.06010890007019\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 10/200 [14:14<4:45:17, 90.09s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  25.366846964372996  eval mae  213.74614238143897  eval ssim  0.5070114920819011  time=  22.771437883377075\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 11/200 [15:37<4:36:59, 87.93s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  10  loss  11.654116535186766  time epoch  83.03566288948059\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 12/200 [17:00<4:30:43, 86.40s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  11  loss  11.15762325525283  time epoch  82.88525128364563\n","output_type":"stream"},{"name":"stderr","text":"  6%|▋         | 13/200 [18:22<4:25:59, 85.35s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  12  loss  10.957501822710032  time epoch  82.9207215309143\n","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 14/200 [19:45<4:22:20, 84.63s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  13  loss  10.539687609672546  time epoch  82.96916556358337\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 15/200 [21:08<4:19:19, 84.10s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  14  loss  10.207694900035861  time epoch  82.88747262954712\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 16/200 [22:31<4:17:02, 83.82s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  15  loss  9.972348362207413  time epoch  83.15480089187622\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 17/200 [23:55<4:14:59, 83.61s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  16  loss  9.771883672475816  time epoch  83.1102523803711\n","output_type":"stream"},{"name":"stderr","text":"  9%|▉         | 18/200 [25:18<4:13:08, 83.45s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  17  loss  9.567624545097354  time epoch  83.0951623916626\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 19/200 [26:41<4:11:23, 83.34s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  18  loss  9.313882583379746  time epoch  83.05985450744629\nepoch  19  loss  8.998328870534898  time epoch  83.03743410110474\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 20/200 [28:27<4:30:29, 90.16s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  29.542836987367533  eval mae  224.48502868078737  eval ssim  0.5410876928536019  time=  23.001746654510498\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 21/200 [29:50<4:22:37, 88.03s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  20  loss  8.825432169437411  time epoch  83.06408905982971\n","output_type":"stream"},{"name":"stderr","text":" 11%|█         | 22/200 [31:13<4:16:45, 86.55s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  21  loss  8.67826247811318  time epoch  83.09179759025574\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 23/200 [32:36<4:12:11, 85.49s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  22  loss  8.442799919843672  time epoch  83.015629529953\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 24/200 [33:59<4:08:33, 84.74s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  23  loss  8.370255285501477  time epoch  82.97607183456421\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▎        | 25/200 [35:22<4:05:36, 84.21s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  24  loss  8.20389031171799  time epoch  82.98443365097046\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 26/200 [36:45<4:03:12, 83.86s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  25  loss  8.021141743659973  time epoch  83.05517983436584\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▎        | 27/200 [38:08<4:01:03, 83.60s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  26  loss  7.946120965480805  time epoch  82.98895001411438\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 28/200 [39:31<3:59:20, 83.49s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  27  loss  7.746756255626679  time epoch  83.22933769226074\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 29/200 [40:54<3:57:34, 83.36s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  28  loss  7.6348612725734695  time epoch  83.05366349220276\nepoch  29  loss  7.406646370887755  time epoch  82.91812562942505\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 30/200 [42:40<4:15:11, 90.07s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  19.974749601487446  eval mae  184.60187545970885  eval ssim  0.5739129104534735  time=  22.773363828659058\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 31/200 [44:03<4:07:36, 87.91s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  30  loss  7.282423692941666  time epoch  82.86849808692932\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 32/200 [45:26<4:01:53, 86.39s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  31  loss  7.244899392127991  time epoch  82.8405294418335\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▋        | 33/200 [46:49<3:57:30, 85.33s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  32  loss  7.078332978487015  time epoch  82.86679697036743\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 34/200 [48:11<3:53:57, 84.57s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  33  loss  7.127733272314073  time epoch  82.77445387840271\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 35/200 [49:34<3:51:13, 84.08s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  34  loss  6.895451366901395  time epoch  82.94213509559631\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 36/200 [50:57<3:48:45, 83.69s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  35  loss  6.9670439720153805  time epoch  82.7945556640625\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 37/200 [52:20<3:46:41, 83.45s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  36  loss  6.916311538219452  time epoch  82.87028074264526\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 38/200 [53:43<3:44:55, 83.31s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  37  loss  6.677091908454893  time epoch  82.97970676422119\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 39/200 [55:06<3:43:30, 83.29s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  38  loss  6.5703058660030385  time epoch  83.26438736915588\nepoch  39  loss  6.470276951789856  time epoch  83.1836256980896\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 40/200 [56:52<4:00:19, 90.12s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  19.489778226708793  eval mae  181.6136941321822  eval ssim  0.5832445255983348  time=  22.83205270767212\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 41/200 [58:15<3:53:03, 87.95s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  40  loss  6.439573687314988  time epoch  82.8770318031311\n","output_type":"stream"},{"name":"stderr","text":" 21%|██        | 42/200 [59:38<3:47:38, 86.44s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  41  loss  6.293355318903919  time epoch  82.93401789665222\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 43/200 [1:01:01<3:43:26, 85.39s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  42  loss  6.237348997592925  time epoch  82.94126534461975\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 44/200 [1:02:24<3:40:10, 84.68s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  43  loss  6.112565249204636  time epoch  83.02056622505188\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▎       | 45/200 [1:03:47<3:37:24, 84.16s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  44  loss  6.131030717492103  time epoch  82.92735528945923\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 46/200 [1:05:10<3:35:06, 83.81s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  45  loss  6.024691745638848  time epoch  83.00178408622742\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▎       | 47/200 [1:06:33<3:33:05, 83.57s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  46  loss  5.952096685767172  time epoch  82.99645400047302\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 48/200 [1:07:56<3:31:13, 83.38s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  47  loss  5.815231963992117  time epoch  82.9432520866394\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 49/200 [1:09:19<3:29:35, 83.28s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  48  loss  5.81156147122383  time epoch  83.04849290847778\nepoch  49  loss  5.71274111866951  time epoch  82.96325182914734\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 50/200 [1:11:05<3:45:18, 90.12s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.161224437054894  eval mae  191.18570369143666  eval ssim  0.567387004726482  time=  23.09364604949951\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 51/200 [1:12:28<3:38:27, 87.97s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  50  loss  5.671284583210947  time epoch  82.93890690803528\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 52/200 [1:13:51<3:33:06, 86.40s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  51  loss  5.529719978570939  time epoch  82.72927927970886\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▋       | 53/200 [1:15:14<3:29:01, 85.32s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  52  loss  5.466522762179376  time epoch  82.80327081680298\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 54/200 [1:16:36<3:25:54, 84.62s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  53  loss  5.466482177376747  time epoch  82.97891402244568\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 55/200 [1:17:59<3:23:09, 84.07s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  54  loss  5.3385376930236825  time epoch  82.78224301338196\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 56/200 [1:19:22<3:20:49, 83.68s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  55  loss  5.246641534566876  time epoch  82.7718436717987\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 57/200 [1:20:45<3:18:40, 83.36s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  56  loss  5.191715970635415  time epoch  82.62450289726257\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▉       | 58/200 [1:22:08<3:16:55, 83.21s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  57  loss  5.1817360728979125  time epoch  82.84739446640015\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 59/200 [1:23:30<3:15:14, 83.08s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  58  loss  5.065925797820092  time epoch  82.7720296382904\nepoch  59  loss  5.073302233219149  time epoch  82.87310528755188\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 60/200 [1:25:16<3:29:45, 89.90s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  24.418881055009024  eval mae  207.28680514420816  eval ssim  0.5604307952285238  time=  22.903430700302124\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 61/200 [1:26:39<3:23:19, 87.77s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  60  loss  5.100867715477946  time epoch  82.79829001426697\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 62/200 [1:28:02<3:18:26, 86.28s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  61  loss  4.901067391037943  time epoch  82.79894924163818\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 63/200 [1:29:25<3:14:43, 85.28s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  62  loss  4.830739018321037  time epoch  82.96021461486816\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 64/200 [1:30:47<3:11:33, 84.51s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  63  loss  4.7799711674451855  time epoch  82.69607853889465\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▎      | 65/200 [1:32:10<3:08:56, 83.97s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  64  loss  4.836532938480376  time epoch  82.72743129730225\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 66/200 [1:33:33<3:06:42, 83.60s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  65  loss  4.693202024698261  time epoch  82.72004866600037\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▎      | 67/200 [1:34:55<3:04:40, 83.32s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  66  loss  4.613938888907433  time epoch  82.65378618240356\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 68/200 [1:36:18<3:02:54, 83.14s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  67  loss  4.599037656188009  time epoch  82.73449802398682\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 69/200 [1:37:41<3:01:14, 83.01s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  68  loss  4.523393604159352  time epoch  82.69787454605103\nepoch  69  loss  4.506723162531853  time epoch  82.72149133682251\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 70/200 [1:39:26<3:14:22, 89.71s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  19.896196026123043  eval mae  182.09819245532464  eval ssim  0.5929634178348308  time=  22.598747730255127\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 71/200 [1:40:49<3:08:18, 87.58s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  70  loss  4.4279025495052355  time epoch  82.61571788787842\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 72/200 [1:42:12<3:03:41, 86.11s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  71  loss  4.413888201117514  time epoch  82.66659307479858\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▋      | 73/200 [1:43:34<3:00:03, 85.06s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  72  loss  4.412725624442103  time epoch  82.62166786193848\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 74/200 [1:44:57<2:57:06, 84.34s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  73  loss  4.396558463573455  time epoch  82.64360523223877\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 75/200 [1:46:19<2:54:38, 83.82s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  74  loss  4.394533699750901  time epoch  82.62366962432861\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 76/200 [1:47:42<2:52:30, 83.47s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  75  loss  4.385345414280892  time epoch  82.64598870277405\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 77/200 [1:49:05<2:50:37, 83.23s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  76  loss  4.382226002216338  time epoch  82.67686772346497\n","output_type":"stream"},{"name":"stderr","text":" 39%|███▉      | 78/200 [1:50:27<2:48:50, 83.04s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  77  loss  4.379563611745832  time epoch  82.57249975204468\n","output_type":"stream"},{"name":"stderr","text":" 40%|███▉      | 79/200 [1:51:50<2:47:14, 82.93s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  78  loss  4.369621455669403  time epoch  82.68258571624756\nepoch  79  loss  4.35564156472683  time epoch  82.65882635116577\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 80/200 [1:53:35<2:59:17, 89.64s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  19.683888271682623  eval mae  178.36042419121654  eval ssim  0.6025908158435054  time=  22.610783576965332\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 81/200 [1:54:58<2:53:39, 87.56s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  80  loss  4.358685180544855  time epoch  82.70932459831238\n","output_type":"stream"},{"name":"stderr","text":" 41%|████      | 82/200 [1:56:21<2:49:16, 86.07s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  81  loss  4.333789169788361  time epoch  82.58466935157776\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 83/200 [1:57:43<2:45:48, 85.03s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  82  loss  4.334989991784096  time epoch  82.61390852928162\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 84/200 [1:59:06<2:42:59, 84.30s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  83  loss  4.324621230363843  time epoch  82.60051465034485\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▎     | 85/200 [2:00:29<2:40:50, 83.91s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  84  loss  4.31522336602211  time epoch  83.00227332115173\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 86/200 [2:01:52<2:38:47, 83.57s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  85  loss  4.299019882082939  time epoch  82.77752232551575\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▎     | 87/200 [2:03:14<2:36:47, 83.25s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  86  loss  4.304493662714957  time epoch  82.50163865089417\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▍     | 88/200 [2:04:37<2:35:00, 83.04s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  87  loss  4.278626680374147  time epoch  82.55661392211914\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▍     | 89/200 [2:06:00<2:33:32, 83.00s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  88  loss  4.282988256216052  time epoch  82.88559126853943\nepoch  89  loss  4.263133040070533  time epoch  82.92275023460388\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 90/200 [2:07:45<2:44:42, 89.84s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.015683286079646  eval mae  179.0577033883536  eval ssim  0.6027655938718669  time=  22.85414958000183\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 91/200 [2:09:08<2:39:25, 87.76s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  90  loss  4.267397806048393  time epoch  82.8982892036438\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 92/200 [2:10:31<2:35:22, 86.32s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  91  loss  4.242937353253365  time epoch  82.94634342193604\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▋     | 93/200 [2:11:54<2:32:04, 85.27s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  92  loss  4.244135695695875  time epoch  82.84061551094055\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 94/200 [2:13:17<2:29:22, 84.56s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  93  loss  4.216614234447479  time epoch  82.88242745399475\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 95/200 [2:14:40<2:27:06, 84.06s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  94  loss  4.21401962041855  time epoch  82.89650893211365\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 96/200 [2:16:03<2:25:03, 83.69s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  95  loss  4.194008332490921  time epoch  82.81397080421448\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 97/200 [2:17:26<2:23:15, 83.45s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  96  loss  4.179788494110108  time epoch  82.89028024673462\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▉     | 98/200 [2:18:48<2:21:33, 83.27s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  97  loss  4.180729851126673  time epoch  82.85785555839539\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 99/200 [2:20:11<2:19:58, 83.15s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  98  loss  4.15275664329529  time epoch  82.86655044555664\nepoch  99  loss  4.1580074846744575  time epoch  83.13238644599915\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 100/200 [2:21:58<2:30:06, 90.07s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.500065316310394  eval mae  180.47409931445898  eval ssim  0.6027407911405952  time=  23.048158884048462\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 101/200 [2:23:21<2:25:10, 87.99s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  100  loss  4.136803871393203  time epoch  83.12140798568726\n","output_type":"stream"},{"name":"stderr","text":" 51%|█████     | 102/200 [2:24:44<2:21:18, 86.52s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  101  loss  4.137809520959857  time epoch  83.09002447128296\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 103/200 [2:26:07<2:18:08, 85.45s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  102  loss  4.134835258126259  time epoch  82.96269130706787\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 104/200 [2:27:30<2:15:30, 84.69s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  103  loss  4.1385493308305765  time epoch  82.91793203353882\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▎    | 105/200 [2:28:52<2:13:13, 84.14s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  104  loss  4.137105199694636  time epoch  82.86413955688477\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 106/200 [2:30:16<2:11:20, 83.84s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  105  loss  4.133338868618014  time epoch  83.11858034133911\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▎    | 107/200 [2:31:38<2:09:27, 83.52s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  106  loss  4.132239651679992  time epoch  82.7664213180542\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 108/200 [2:33:01<2:07:42, 83.29s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  107  loss  4.135428556799889  time epoch  82.77181696891785\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▍    | 109/200 [2:34:24<2:06:04, 83.12s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  108  loss  4.13126765191555  time epoch  82.7189028263092\nepoch  109  loss  4.1257226973772045  time epoch  82.72488808631897\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 110/200 [2:36:09<2:14:42, 89.81s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.38212116004333  eval mae  180.02484022762192  eval ssim  0.6022605786955287  time=  22.664498805999756\n","output_type":"stream"},{"name":"stderr","text":" 56%|█████▌    | 111/200 [2:37:32<2:10:04, 87.69s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  110  loss  4.129494208097458  time epoch  82.74765729904175\n","output_type":"stream"},{"name":"stderr","text":" 56%|█████▌    | 112/200 [2:38:55<2:06:22, 86.16s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  111  loss  4.123183801770209  time epoch  82.5857207775116\n","output_type":"stream"},{"name":"stderr","text":" 56%|█████▋    | 113/200 [2:40:17<2:03:28, 85.15s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  112  loss  4.123659694194794  time epoch  82.79704260826111\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 114/200 [2:41:40<2:00:57, 84.39s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  113  loss  4.123670974373817  time epoch  82.59784245491028\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▊    | 115/200 [2:43:03<1:58:49, 83.87s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  114  loss  4.126704987883568  time epoch  82.67317938804626\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 116/200 [2:44:25<1:56:54, 83.50s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  115  loss  4.113927081227303  time epoch  82.64000988006592\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 117/200 [2:45:48<1:55:09, 83.25s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  116  loss  4.123156079649925  time epoch  82.65996146202087\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 118/200 [2:47:11<1:53:31, 83.07s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  117  loss  4.120359358191492  time epoch  82.65452885627747\n","output_type":"stream"},{"name":"stderr","text":" 60%|█████▉    | 119/200 [2:48:33<1:51:57, 82.93s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  118  loss  4.115887671709059  time epoch  82.61011385917664\nepoch  119  loss  4.118467319011689  time epoch  82.66501331329346\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 120/200 [2:50:19<1:59:30, 89.63s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.45201426639322  eval mae  180.26802719537025  eval ssim  0.6024137987758376  time=  22.571511030197144\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 121/200 [2:51:41<1:55:14, 87.52s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  120  loss  4.123876431584358  time epoch  82.60002017021179\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████    | 122/200 [2:53:04<1:51:51, 86.04s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  121  loss  4.113316768407821  time epoch  82.58565592765808\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 123/200 [2:54:26<1:49:05, 85.01s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  122  loss  4.110655492544176  time epoch  82.60496211051941\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 124/200 [2:55:49<1:46:48, 84.32s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  123  loss  4.112520274519921  time epoch  82.7008969783783\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▎   | 125/200 [2:57:12<1:44:42, 83.77s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  124  loss  4.103931888937949  time epoch  82.4942696094513\n","output_type":"stream"},{"name":"stderr","text":" 63%|██████▎   | 126/200 [2:58:34<1:42:50, 83.38s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  125  loss  4.106375795602798  time epoch  82.47521567344666\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▎   | 127/200 [2:59:56<1:41:07, 83.11s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  126  loss  4.0965618789196006  time epoch  82.47836375236511\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▍   | 128/200 [3:01:19<1:39:31, 82.94s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  127  loss  4.110226556658745  time epoch  82.5212230682373\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▍   | 129/200 [3:02:42<1:37:59, 82.81s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  128  loss  4.1005742192268375  time epoch  82.52722454071045\nepoch  129  loss  4.098058357834816  time epoch  82.50554060935974\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▌   | 130/200 [3:04:27<1:44:26, 89.52s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.461360421853538  eval mae  180.16810672317357  eval ssim  0.6020201041514779  time=  22.628646850585938\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▌   | 131/200 [3:05:49<1:40:33, 87.44s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  130  loss  4.09052456021309  time epoch  82.59286856651306\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▌   | 132/200 [3:07:12<1:37:26, 85.98s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  131  loss  4.101493248343467  time epoch  82.55817866325378\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▋   | 133/200 [3:08:34<1:34:52, 84.96s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  132  loss  4.10227136015892  time epoch  82.59801197052002\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 134/200 [3:09:57<1:32:40, 84.25s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  133  loss  4.100677055120468  time epoch  82.58063864707947\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 135/200 [3:11:20<1:30:43, 83.74s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  134  loss  4.0952956706285475  time epoch  82.54866027832031\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 136/200 [3:12:42<1:28:56, 83.38s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  135  loss  4.1008059144020095  time epoch  82.55395102500916\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 137/200 [3:14:05<1:27:20, 83.18s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  136  loss  4.103809195756912  time epoch  82.7008113861084\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 138/200 [3:15:27<1:25:47, 83.02s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  137  loss  4.095106986165047  time epoch  82.63635730743408\n","output_type":"stream"},{"name":"stderr","text":" 70%|██████▉   | 139/200 [3:16:50<1:24:16, 82.89s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  138  loss  4.099648013710976  time epoch  82.57925462722778\nepoch  139  loss  4.1105956703424456  time epoch  82.4810619354248\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 140/200 [3:18:35<1:29:32, 89.54s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.513727243233657  eval mae  180.25702346318576  eval ssim  0.6025281771177821  time=  22.545645713806152\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 141/200 [3:19:58<1:25:58, 87.43s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  140  loss  4.100688415765765  time epoch  82.49700498580933\n","output_type":"stream"},{"name":"stderr","text":" 71%|███████   | 142/200 [3:21:20<1:23:05, 85.96s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  141  loss  4.107346284389496  time epoch  82.55228734016418\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 143/200 [3:22:43<1:20:40, 84.93s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  142  loss  4.10295777320862  time epoch  82.49746680259705\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 144/200 [3:24:05<1:18:34, 84.19s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  143  loss  4.094086304306984  time epoch  82.4779405593872\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▎  | 145/200 [3:25:28<1:16:44, 83.71s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  144  loss  4.101066923141478  time epoch  82.59172677993774\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 146/200 [3:26:50<1:15:01, 83.37s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  145  loss  4.1022306889295574  time epoch  82.560063123703\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▎  | 147/200 [3:28:13<1:13:25, 83.12s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  146  loss  4.10077896416187  time epoch  82.53672170639038\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 148/200 [3:29:35<1:11:53, 82.95s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  147  loss  4.09946447312832  time epoch  82.54132747650146\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 149/200 [3:30:58<1:10:23, 82.81s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  148  loss  4.101975050568583  time epoch  82.4794979095459\nepoch  149  loss  4.097974467277524  time epoch  82.56442785263062\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 150/200 [3:32:43<1:14:34, 89.50s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.49413575373794  eval mae  180.2377822947954  eval ssim  0.6025435884624747  time=  22.514856576919556\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 151/200 [3:34:06<1:11:23, 87.42s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  150  loss  4.104212442040445  time epoch  82.55867552757263\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 152/200 [3:35:28<1:08:45, 85.94s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  151  loss  4.112181219458579  time epoch  82.5059130191803\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▋  | 153/200 [3:36:51<1:06:30, 84.91s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  152  loss  4.098515507578851  time epoch  82.48847436904907\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 154/200 [3:38:13<1:04:32, 84.19s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  153  loss  4.099141773581504  time epoch  82.53061366081238\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 155/200 [3:39:35<1:02:44, 83.66s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  154  loss  4.104522299766541  time epoch  82.40650987625122\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 156/200 [3:40:58<1:01:06, 83.33s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  155  loss  4.113370403647424  time epoch  82.56056213378906\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 157/200 [3:42:20<59:31, 83.07s/it]  ","output_type":"stream"},{"name":"stdout","text":"epoch  156  loss  4.104977908730508  time epoch  82.44697523117065\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 158/200 [3:43:43<58:02, 82.92s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  157  loss  4.105924460291862  time epoch  82.58414840698242\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 159/200 [3:45:06<56:34, 82.79s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  158  loss  4.108896598219871  time epoch  82.49127960205078\nepoch  159  loss  4.102937623858452  time epoch  82.5196316242218\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 160/200 [3:46:51<59:38, 89.46s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.513110505181857  eval mae  180.26327800352203  eval ssim  0.6025020643658366  time=  22.45680284500122\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 161/200 [3:48:13<56:47, 87.37s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  160  loss  4.120111766457559  time epoch  82.49202299118042\n","output_type":"stream"},{"name":"stderr","text":" 81%|████████  | 162/200 [3:49:36<54:25, 85.93s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  161  loss  4.110804328322409  time epoch  82.55754280090332\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 163/200 [3:50:58<52:20, 84.89s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  162  loss  4.10560349225998  time epoch  82.47736263275146\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 164/200 [3:52:21<50:29, 84.16s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  163  loss  4.098942026495933  time epoch  82.44434571266174\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▎ | 165/200 [3:53:43<48:48, 83.66s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  164  loss  4.10663000047207  time epoch  82.5075933933258\n","output_type":"stream"},{"name":"stderr","text":" 83%|████████▎ | 166/200 [3:55:06<47:13, 83.33s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  165  loss  4.116805288195612  time epoch  82.544504404068\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▎ | 167/200 [3:56:28<45:41, 83.07s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  166  loss  4.110636338591576  time epoch  82.47596716880798\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 168/200 [3:57:51<44:12, 82.89s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  167  loss  4.099751546978951  time epoch  82.44954586029053\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 169/200 [3:59:13<42:45, 82.75s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  168  loss  4.109323477745057  time epoch  82.41638922691345\nepoch  169  loss  4.106326788663864  time epoch  82.3900830745697\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▌ | 170/200 [4:00:58<44:41, 89.38s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.518553323148367  eval mae  180.2834730193711  eval ssim  0.6024024772587422  time=  22.44735312461853\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 171/200 [4:02:20<42:12, 87.32s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  170  loss  4.110209396481514  time epoch  82.5121841430664\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 172/200 [4:03:43<40:03, 85.85s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  171  loss  4.108400875329973  time epoch  82.4166419506073\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▋ | 173/200 [4:05:05<38:10, 84.83s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  172  loss  4.105450427532197  time epoch  82.44637155532837\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 174/200 [4:06:28<36:27, 84.13s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  173  loss  4.112995579838753  time epoch  82.4874176979065\n","output_type":"stream"},{"name":"stderr","text":" 88%|████████▊ | 175/200 [4:07:50<34:50, 83.62s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  174  loss  4.110492345690725  time epoch  82.43843483924866\n","output_type":"stream"},{"name":"stderr","text":" 88%|████████▊ | 176/200 [4:09:13<33:18, 83.27s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  175  loss  4.0989005535841  time epoch  82.45656895637512\n","output_type":"stream"},{"name":"stderr","text":" 88%|████████▊ | 177/200 [4:10:35<31:50, 83.05s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  176  loss  4.100954782962799  time epoch  82.5304388999939\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 178/200 [4:11:58<30:23, 82.87s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  177  loss  4.102192309498787  time epoch  82.44217658042908\n","output_type":"stream"},{"name":"stderr","text":" 90%|████████▉ | 179/200 [4:13:20<28:57, 82.76s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  178  loss  4.105202215909958  time epoch  82.50632810592651\nepoch  179  loss  4.108557745814323  time epoch  82.45974564552307\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 180/200 [4:15:05<29:48, 89.43s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.520081400676695  eval mae  180.29428405204166  eval ssim  0.6023670439632473  time=  22.486343145370483\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 181/200 [4:16:27<27:39, 87.33s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  180  loss  4.111550232768058  time epoch  82.43606233596802\n","output_type":"stream"},{"name":"stderr","text":" 91%|█████████ | 182/200 [4:17:50<25:45, 85.87s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  181  loss  4.113076478242874  time epoch  82.47570872306824\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 183/200 [4:19:12<24:02, 84.85s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  182  loss  4.116122376918794  time epoch  82.45768141746521\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 184/200 [4:20:35<22:26, 84.15s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  183  loss  4.123499527573584  time epoch  82.50877857208252\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▎| 185/200 [4:21:57<20:54, 83.65s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  184  loss  4.1179613232612615  time epoch  82.48551321029663\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 186/200 [4:23:20<19:26, 83.30s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  185  loss  4.114222162961961  time epoch  82.47043919563293\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▎| 187/200 [4:24:42<17:59, 83.07s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  186  loss  4.108787357807161  time epoch  82.55051445960999\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▍| 188/200 [4:26:05<16:34, 82.89s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  187  loss  4.111010414361953  time epoch  82.45599365234375\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▍| 189/200 [4:27:27<15:10, 82.75s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  188  loss  4.1100010693073274  time epoch  82.42895150184631\nepoch  189  loss  4.125945496559142  time epoch  82.43234825134277\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 190/200 [4:29:12<14:53, 89.39s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.533290700573694  eval mae  180.33760778749576  eval ssim  0.6022884715673198  time=  22.410666704177856\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 191/200 [4:30:35<13:05, 87.31s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  190  loss  4.114041548967361  time epoch  82.4724771976471\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 192/200 [4:31:57<11:26, 85.84s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  191  loss  4.112463149428367  time epoch  82.4051923751831\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▋| 193/200 [4:33:20<09:53, 84.82s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  192  loss  4.120964458584785  time epoch  82.42832684516907\n","output_type":"stream"},{"name":"stderr","text":" 97%|█████████▋| 194/200 [4:34:42<08:24, 84.12s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  193  loss  4.114839631319045  time epoch  82.50529336929321\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 195/200 [4:36:04<06:58, 83.62s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  194  loss  4.119736760854722  time epoch  82.43307733535767\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 196/200 [4:37:27<05:33, 83.29s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  195  loss  4.119505396485329  time epoch  82.51364064216614\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 197/200 [4:38:49<04:09, 83.03s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  196  loss  4.12648673951626  time epoch  82.44149398803711\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 198/200 [4:40:12<02:45, 82.89s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  197  loss  4.105794262886048  time epoch  82.56826877593994\n","output_type":"stream"},{"name":"stderr","text":"100%|█████████▉| 199/200 [4:41:35<01:22, 82.78s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  198  loss  4.119596847891808  time epoch  82.52569556236267\nepoch  199  loss  4.114895778894424  time epoch  82.46853470802307\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 200/200 [4:43:19<00:00, 85.00s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  20.535118786559313  eval mae  180.34288546097875  eval ssim  0.6022884795168806  time=  22.400813817977905\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4}]}