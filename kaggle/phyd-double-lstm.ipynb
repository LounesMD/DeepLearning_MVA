{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10290336,"sourceType":"datasetVersion","datasetId":6368558}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport numpy as np\nimport random\nimport time\nfrom tqdm import tqdm\nfrom skimage.metrics import structural_similarity as ssim\nimport matplotlib.pyplot as plt\nimport torch.utils.data as data\n\nimport argparse\nimport os\nimport gzip\nimport numpy as np\n##############################################################################################################\ndef load_mnist(root='/kaggle/mnist-dataset/data'):\n    \"\"\"\n    Load the MNIST dataset for generating training data.\n    Adapted to work in Kaggle notebooks.\n    \"\"\"\n    path = os.path.join('', '/kaggle/input/mnist-dataset/data/train-images-idx3-ubyte/train-images.idx3-ubyte')\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"File not found: {path}. Ensure the MNIST dataset is uploaded or available in {root}.\")\n    \n    with open(path, 'rb') as f:\n        mnist = np.frombuffer(f.read(), np.uint8, offset=16)\n        mnist = mnist.reshape(-1, 28, 28)\n    return mnist\n\n\ndef load_fixed_set(root, is_train):\n    # Load the fixed dataset\n    filename = 'mnist_test_seq.npy'\n    path = os.path.join('', \"/kaggle/input/mnist-dataset/data/mnist_test_seq.npy\")\n    dataset = np.load(path)\n    dataset = dataset[..., np.newaxis]\n    return dataset\n\n\nclass MovingMNIST(data.Dataset):\n    def __init__(self, root, is_train=True, n_frames_input=10, n_frames_output=10, num_objects=[2],\n                 transform=None):\n        '''\n        param num_objects: a list of number of possible objects.\n        '''\n        super(MovingMNIST, self).__init__()\n\n        self.dataset = None\n        if is_train:\n            self.mnist = load_mnist(root)\n        else:\n            if num_objects[0] != 2:\n                self.mnist = load_mnist(root)\n            else:\n                self.dataset = load_fixed_set(root, False)\n        self.length = int(1e4) if self.dataset is None else self.dataset.shape[1]\n\n        self.is_train = is_train\n        self.num_objects = num_objects\n        self.n_frames_input = n_frames_input\n        self.n_frames_output = n_frames_output\n        self.n_frames_total = self.n_frames_input + self.n_frames_output\n        self.transform = transform\n        # For generating data\n        self.image_size_ = 64\n        self.digit_size_ = 28\n        self.step_length_ = 0.1\n\n    def get_random_trajectory(self, seq_length):\n        ''' Generate a random sequence of a MNIST digit '''\n        canvas_size = self.image_size_ - self.digit_size_\n        x = random.random()\n        y = random.random()\n        theta = random.random() * 2 * np.pi\n        v_y = np.sin(theta)\n        v_x = np.cos(theta)\n\n        start_y = np.zeros(seq_length)\n        start_x = np.zeros(seq_length)\n        for i in range(seq_length):\n            # Take a step along velocity.\n            y += v_y * self.step_length_\n            x += v_x * self.step_length_\n\n            # Bounce off edges.\n            if x <= 0:\n                x = 0\n                v_x = -v_x\n            if x >= 1.0:\n                x = 1.0\n                v_x = -v_x\n            if y <= 0:\n                y = 0\n                v_y = -v_y\n            if y >= 1.0:\n                y = 1.0\n                v_y = -v_y\n            start_y[i] = y\n            start_x[i] = x\n\n        # Scale to the size of the canvas.\n        start_y = (canvas_size * start_y).astype(np.int32)\n        start_x = (canvas_size * start_x).astype(np.int32)\n        return start_y, start_x\n\n    def generate_moving_mnist(self, num_digits=2):\n        '''\n        Get random trajectories for the digits and generate a video.\n        '''\n        data = np.zeros((self.n_frames_total, self.image_size_, self.image_size_), dtype=np.float32)\n        for n in range(num_digits):\n            # Trajectory\n            start_y, start_x = self.get_random_trajectory(self.n_frames_total)\n            ind = random.randint(0, self.mnist.shape[0] - 1)\n            digit_image = self.mnist[ind]\n            for i in range(self.n_frames_total):\n                top = start_y[i]\n                left = start_x[i]\n                bottom = top + self.digit_size_\n                right = left + self.digit_size_\n                # Draw digit\n                data[i, top:bottom, left:right] = np.maximum(data[i, top:bottom, left:right], digit_image)\n\n        data = data[..., np.newaxis]\n        return data\n\n    def __getitem__(self, idx):\n        length = self.n_frames_input + self.n_frames_output\n        if self.is_train or self.num_objects[0] != 2:\n            # Sample number of objects\n            num_digits = random.choice(self.num_objects)\n            # Generate data on the fly\n            images = self.generate_moving_mnist(num_digits)\n        else:\n            images = self.dataset[:, idx, ...]\n\n        # if self.transform is not None:\n        #     images = self.transform(images)\n\n        r = 1 # patch size (a 4 dans les PredRNN)\n        w = int(64 / r)\n        images = images.reshape((length, w, r, w, r)).transpose(0, 2, 4, 1, 3).reshape((length, r * r, w, w))\n\n        input = images[:self.n_frames_input]\n        if self.n_frames_output > 0:\n            output = images[self.n_frames_input:length]\n        else:\n            output = []\n\n        frozen = input[-1]\n        # add a wall to input data\n        # pad = np.zeros_like(input[:, 0])\n        # pad[:, 0] = 1\n        # pad[:, pad.shape[1] - 1] = 1\n        # pad[:, :, 0] = 1\n        # pad[:, :, pad.shape[2] - 1] = 1\n        #\n        # input = np.concatenate((input, np.expand_dims(pad, 1)), 1)\n\n        output = torch.from_numpy(output / 255.0).contiguous().float()\n        input = torch.from_numpy(input / 255.0).contiguous().float()\n        # print()\n        # print(input.size())\n        # print(output.size())\n\n        out = [idx,input,output]\n        return out\n\n    def __len__(self):\n        return self.length\n\n##############################################################################################################\n    \nfrom numpy import *\nfrom numpy.linalg import *\nfrom scipy.special import factorial\nfrom functools import reduce\nimport torch\nimport torch.nn as nn\n\nfrom functools import reduce\n\n__all__ = ['M2K','K2M']\n\ndef _apply_axis_left_dot(x, mats):\n    assert x.dim() == len(mats)+1\n    sizex = x.size()\n    k = x.dim()-1\n    for i in range(k):\n        x = tensordot(mats[k-i-1], x, dim=[1,k])\n    x = x.permute([k,]+list(range(k))).contiguous()\n    x = x.view(sizex)\n    return x\n\ndef _apply_axis_right_dot(x, mats):\n    assert x.dim() == len(mats)+1\n    sizex = x.size()\n    k = x.dim()-1\n    x = x.permute(list(range(1,k+1))+[0,])\n    for i in range(k):\n        x = tensordot(x, mats[i], dim=[0,0])\n    x = x.contiguous()\n    x = x.view(sizex)\n    return x\n\nclass _MK(nn.Module):\n    def __init__(self, shape):\n        super(_MK, self).__init__()\n        self._size = torch.Size(shape)\n        self._dim = len(shape)\n        M = []\n        invM = []\n        assert len(shape) > 0\n        j = 0\n        for l in shape:\n            M.append(zeros((l,l)))\n            for i in range(l):\n                M[-1][i] = ((arange(l)-(l-1)//2)**i)/factorial(i)\n            invM.append(inv(M[-1]))\n            self.register_buffer('_M'+str(j), torch.from_numpy(M[-1]))\n            self.register_buffer('_invM'+str(j), torch.from_numpy(invM[-1]))\n            j += 1\n\n    @property\n    def M(self):\n        return list(self._buffers['_M'+str(j)] for j in range(self.dim()))\n    @property\n    def invM(self):\n        return list(self._buffers['_invM'+str(j)] for j in range(self.dim()))\n\n    def size(self):\n        return self._size\n    def dim(self):\n        return self._dim\n    def _packdim(self, x):\n        assert x.dim() >= self.dim()\n        if x.dim() == self.dim():\n            x = x[newaxis,:]\n        x = x.contiguous()\n        x = x.view([-1,]+list(x.size()[-self.dim():]))\n        return x\n\n    def forward(self):\n        pass\n\nclass M2K(_MK):\n    \"\"\"\n    convert moment matrix to convolution kernel\n    Arguments:\n        shape (tuple of int): kernel shape\n    Usage:\n        m2k = M2K([5,5])\n        m = torch.randn(5,5,dtype=torch.float64)\n        k = m2k(m)\n    \"\"\"\n    def __init__(self, shape):\n        super(M2K, self).__init__(shape)\n    def forward(self, m):\n        \"\"\"\n        m (Tensor): torch.size=[...,*self.shape]\n        \"\"\"\n        sizem = m.size()\n        m = self._packdim(m)\n        m = _apply_axis_left_dot(m, self.invM)\n        m = m.view(sizem)\n        return m\n\nclass K2M(_MK):\n    \"\"\"\n    convert convolution kernel to moment matrix\n    Arguments:\n        shape (tuple of int): kernel shape\n    Usage:\n        k2m = K2M([5,5])\n        k = torch.randn(5,5,dtype=torch.float64)\n        m = k2m(k)\n    \"\"\"\n    def __init__(self, shape):\n        super(K2M, self).__init__(shape)\n    def forward(self, k):\n        \"\"\"\n        k (Tensor): torch.size=[...,*self.shape]\n        \"\"\"\n        sizek = k.size()\n        k = self._packdim(k)\n        k = _apply_axis_left_dot(k, self.M)\n        k = k.view(sizek)\n        return k\n\n\n    \ndef tensordot(a,b,dim):\n    \"\"\"\n    tensordot in PyTorch, see numpy.tensordot?\n    \"\"\"\n    l = lambda x,y:x*y\n    if isinstance(dim,int):\n        a = a.contiguous()\n        b = b.contiguous()\n        sizea = a.size()\n        sizeb = b.size()\n        sizea0 = sizea[:-dim]\n        sizea1 = sizea[-dim:]\n        sizeb0 = sizeb[:dim]\n        sizeb1 = sizeb[dim:]\n        N = reduce(l, sizea1, 1)\n        assert reduce(l, sizeb0, 1) == N\n    else:\n        adims = dim[0]\n        bdims = dim[1]\n        adims = [adims,] if isinstance(adims, int) else adims\n        bdims = [bdims,] if isinstance(bdims, int) else bdims\n        adims_ = set(range(a.dim())).difference(set(adims))\n        adims_ = list(adims_)\n        adims_.sort()\n        perma = adims_+adims\n        bdims_ = set(range(b.dim())).difference(set(bdims))\n        bdims_ = list(bdims_)\n        bdims_.sort()\n        permb = bdims+bdims_\n        a = a.permute(*perma).contiguous()\n        b = b.permute(*permb).contiguous()\n\n        sizea = a.size()\n        sizeb = b.size()\n        sizea0 = sizea[:-len(adims)]\n        sizea1 = sizea[-len(adims):]\n        sizeb0 = sizeb[:len(bdims)]\n        sizeb1 = sizeb[len(bdims):]\n        N = reduce(l, sizea1, 1)\n        assert reduce(l, sizeb0, 1) == N\n    a = a.view([-1,N])\n    b = b.view([N,-1])\n    c = a@b\n    return c.view(sizea0+sizeb1)\n\n##############################################################################################################\nimport torch\nimport torch.nn as nn\n\nclass PhyCell_Cell(nn.Module):\n    def __init__(self, input_dim, F_hidden_dim, kernel_size, bias=1):\n        super(PhyCell_Cell, self).__init__()\n        self.input_dim  = input_dim\n        self.F_hidden_dim = F_hidden_dim\n        self.kernel_size = kernel_size\n        self.padding     = kernel_size[0] // 2, kernel_size[1] // 2\n        self.bias = bias\n        \n        self.F = nn.Sequential()\n        self.F.add_module('conv1', nn.Conv2d(in_channels=input_dim, out_channels=F_hidden_dim, kernel_size=self.kernel_size, stride=(1,1), padding=self.padding))\n        self.F.add_module('bn1',nn.GroupNorm( 7 ,F_hidden_dim))        \n        self.F.add_module('conv2', nn.Conv2d(in_channels=F_hidden_dim, out_channels=input_dim, kernel_size=(1,1), stride=(1,1), padding=(0,0)))\n\n        self.convgate = nn.Conv2d(in_channels=self.input_dim + self.input_dim,\n                              out_channels= self.input_dim,\n                              kernel_size=(3,3),\n                              padding=(1,1), bias=self.bias)\n\n    def forward(self, x, hidden): # x [batch_size, hidden_dim, height, width]      \n        combined = torch.cat([x, hidden], dim=1)  # concatenate along channel axis\n        combined_conv = self.convgate(combined)\n        K = torch.sigmoid(combined_conv)\n        hidden_tilde = hidden + self.F(hidden)        # prediction\n        next_hidden = hidden_tilde + K * (x-hidden_tilde)   # correction , Haddamard product     \n        return next_hidden\n\nclass PhyCell(nn.Module):\n    def __init__(self, input_shape, input_dim, F_hidden_dims, n_layers, kernel_size, device):\n        super(PhyCell, self).__init__()\n        self.input_shape = input_shape\n        self.input_dim = input_dim\n        self.F_hidden_dims = F_hidden_dims\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.H = []  \n        self.device = device\n             \n        cell_list = []\n        for i in range(0, self.n_layers):\n            cell_list.append(PhyCell_Cell(input_dim=input_dim,\n                                          F_hidden_dim=self.F_hidden_dims[i],\n                                          kernel_size=self.kernel_size))                                     \n        self.cell_list = nn.ModuleList(cell_list)\n        \n       \n    def forward(self, input_, first_timestep=False): # input_ [batch_size, 1, channels, width, height]    \n        batch_size = input_.data.size()[0]\n        if (first_timestep):   \n            self.initHidden(batch_size) # init Hidden at each forward start\n              \n        for j,cell in enumerate(self.cell_list):\n            if j==0: # bottom layer\n                self.H[j] = cell(input_, self.H[j])\n            else:\n                self.H[j] = cell(self.H[j-1],self.H[j])\n        \n        return self.H , self.H \n    \n    def initHidden(self,batch_size):\n        self.H = [] \n        for i in range(self.n_layers):\n            self.H.append( torch.zeros(batch_size, self.input_dim, self.input_shape[0], self.input_shape[1]).to(self.device) )\n\n    def setHidden(self, H):\n        self.H = H\n\n        \nclass ConvLSTM_Cell(nn.Module):\n    def __init__(self, input_shape, input_dim, hidden_dim, kernel_size, bias=1):              \n        \"\"\"\n        input_shape: (int, int)\n            Height and width of input tensor as (height, width).\n        input_dim: int\n            Number of channels of input tensor.\n        hidden_dim: int\n            Number of channels of hidden state.\n        kernel_size: (int, int)\n            Size of the convolutional kernel.\n        bias: bool\n            Whether or not to add the bias.\n        \"\"\"\n        super(ConvLSTM_Cell, self).__init__()\n        \n        self.height, self.width = input_shape\n        self.input_dim  = input_dim\n        self.hidden_dim = hidden_dim\n        self.kernel_size = kernel_size\n        self.padding     = kernel_size[0] // 2, kernel_size[1] // 2\n        self.bias        = bias\n        \n        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n                              out_channels=4 * self.hidden_dim,\n                              kernel_size=self.kernel_size,\n                              padding=self.padding, bias=self.bias)\n                 \n    # we implement LSTM that process only one timestep \n    def forward(self,x, hidden): # x [batch, hidden_dim, width, height]          \n        h_cur, c_cur = hidden\n        \n        combined = torch.cat([x, h_cur], dim=1)  # concatenate along channel axis\n        combined_conv = self.conv(combined)\n        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1) \n        i = torch.sigmoid(cc_i)\n        f = torch.sigmoid(cc_f)\n        o = torch.sigmoid(cc_o)\n        g = torch.tanh(cc_g)\n\n        c_next = f * c_cur + i * g\n        h_next = o * torch.tanh(c_next)\n        return h_next, c_next\n\n\nclass ConvLSTM(nn.Module):\n    def __init__(self, input_shape, input_dim, hidden_dims, n_layers, kernel_size,device):\n        super(ConvLSTM, self).__init__()\n        self.input_shape = input_shape\n        self.input_dim = input_dim\n        self.hidden_dims = hidden_dims\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.H, self.C = [],[]   \n        self.device = device\n        \n        cell_list = []\n        for i in range(0, self.n_layers):\n            cur_input_dim = self.input_dim if i == 0 else self.hidden_dims[i-1]\n            print('layer ',i,'input dim ', cur_input_dim, ' hidden dim ', self.hidden_dims[i])\n            cell_list.append(ConvLSTM_Cell(input_shape=self.input_shape,\n                                          input_dim=cur_input_dim,\n                                          hidden_dim=self.hidden_dims[i],\n                                          kernel_size=self.kernel_size))                                     \n        self.cell_list = nn.ModuleList(cell_list)\n        \n       \n    def forward(self, input_, first_timestep=False): # input_ [batch_size, 1, channels, width, height]    \n        batch_size = input_.data.size()[0]\n        if (first_timestep):   \n            self.initHidden(batch_size) # init Hidden at each forward start\n              \n        for j,cell in enumerate(self.cell_list):\n            if j==0: # bottom layer\n                self.H[j], self.C[j] = cell(input_, (self.H[j],self.C[j]))\n            else:\n                self.H[j], self.C[j] = cell(self.H[j-1],(self.H[j],self.C[j]))\n        \n        return (self.H,self.C) , self.H   # (hidden, output)\n    \n    def initHidden(self,batch_size):\n        self.H, self.C = [],[]  \n        for i in range(self.n_layers):\n            self.H.append( torch.zeros(batch_size,self.hidden_dims[i], self.input_shape[0], self.input_shape[1]).to(self.device) )\n            self.C.append( torch.zeros(batch_size,self.hidden_dims[i], self.input_shape[0], self.input_shape[1]).to(self.device) )\n    \n    def setHidden(self, hidden):\n        H,C = hidden\n        self.H, self.C = H,C\n \n\nclass dcgan_conv(nn.Module):\n    def __init__(self, nin, nout, stride):\n        super(dcgan_conv, self).__init__()\n        self.main = nn.Sequential(\n                nn.Conv2d(in_channels=nin, out_channels=nout, kernel_size=(3,3), stride=stride, padding=1),\n                nn.GroupNorm(16,nout),\n                nn.LeakyReLU(0.2, inplace=True),\n                )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass dcgan_upconv(nn.Module):\n    def __init__(self, nin, nout, stride):\n        super(dcgan_upconv, self).__init__()\n        if (stride ==2):\n            output_padding = 1\n        else:\n            output_padding = 0\n        self.main = nn.Sequential(\n                nn.ConvTranspose2d(in_channels=nin,out_channels=nout,kernel_size=(3,3), stride=stride,padding=1,output_padding=output_padding),\n                nn.GroupNorm(16,nout),\n                nn.LeakyReLU(0.2, inplace=True),\n                )\n\n    def forward(self, input):\n        return self.main(input)\n        \nclass encoder_E(nn.Module):\n    def __init__(self, nc=1, nf=32):\n        super(encoder_E, self).__init__()\n        # input is (1) x 64 x 64\n        self.c1 = dcgan_conv(nc, nf, stride=2) # (32) x 32 x 32\n        self.c2 = dcgan_conv(nf, nf, stride=1) # (32) x 32 x 32\n        self.c3 = dcgan_conv(nf, 2*nf, stride=2) # (64) x 16 x 16\n\n    def forward(self, input):\n        h1 = self.c1(input)  \n        h2 = self.c2(h1)    \n        h3 = self.c3(h2)\n        return h3\n\nclass decoder_D(nn.Module):\n    def __init__(self, nc=1, nf=32):\n        super(decoder_D, self).__init__()\n        self.upc1 = dcgan_upconv(2*nf, nf, stride=2) #(32) x 32 x 32\n        self.upc2 = dcgan_upconv(nf, nf, stride=1) #(32) x 32 x 32\n        self.upc3 = nn.ConvTranspose2d(in_channels=nf,out_channels=nc,kernel_size=(3,3),stride=2,padding=1,output_padding=1)  #(nc) x 64 x 64\n\n    def forward(self, input):      \n        d1 = self.upc1(input) \n        d2 = self.upc2(d1)\n        d3 = self.upc3(d2)  \n        return d3  \n\n\nclass encoder_specific(nn.Module):\n    def __init__(self, nc=64, nf=64):\n        super(encoder_specific, self).__init__()\n        self.c1 = dcgan_conv(nc, nf, stride=1) # (64) x 16 x 16\n        self.c2 = dcgan_conv(nf, nf, stride=1) # (64) x 16 x 16\n\n    def forward(self, input):\n        h1 = self.c1(input)  \n        h2 = self.c2(h1)     \n        return h2\n\nclass decoder_specific(nn.Module):\n    def __init__(self, nc=64, nf=64):\n        super(decoder_specific, self).__init__()\n        self.upc1 = dcgan_upconv(nf, nf, stride=1) #(64) x 16 x 16\n        self.upc2 = dcgan_upconv(nf, nc, stride=1) #(32) x 32 x 32\n        \n    def forward(self, input):\n        d1 = self.upc1(input) \n        d2 = self.upc2(d1)  \n        return d2       \n\n        \nclass EncoderRNN(torch.nn.Module):\n    def __init__(self,phycell,convcell, device):\n        super(EncoderRNN, self).__init__()\n        self.encoder_E = encoder_E()   # general encoder 64x64x1 -> 32x32x32\n        self.encoder_Ep = encoder_specific() # specific image encoder 32x32x32 -> 16x16x64\n        self.encoder_Er = encoder_specific() \n        self.decoder_Dp = decoder_specific() # specific image decoder 16x16x64 -> 32x32x32 \n        self.decoder_Dr = decoder_specific()     \n        self.decoder_D = decoder_D()  # general decoder 32x32x32 -> 64x64x1 \n\n        self.encoder_E = self.encoder_E.to(device)\n        self.encoder_Ep = self.encoder_Ep.to(device) \n        self.encoder_Er = self.encoder_Er.to(device) \n        self.decoder_Dp = self.decoder_Dp.to(device) \n        self.decoder_Dr = self.decoder_Dr.to(device)               \n        self.decoder_D = self.decoder_D.to(device)\n        self.phycell = phycell.to(device)\n        self.convcell = convcell.to(device)\n\n    def forward(self, input, first_timestep=False, decoding=False):\n        input = self.encoder_E(input) # general encoder 64x64x1 -> 32x32x32\n    \n        if decoding:  # input=None in decoding phase\n            input_phys = None\n        else:\n            input_phys = self.encoder_Ep(input)\n        input_conv = self.encoder_Er(input)     \n\n        hidden1, output1 = self.phycell(input_phys, first_timestep)\n        hidden2, output2 = self.convcell(input_conv, first_timestep)\n\n        decoded_Dp = self.decoder_Dp(output1[-1])\n        decoded_Dr = self.decoder_Dr(output2[-1])\n        \n        out_phys = torch.sigmoid(self.decoder_D(decoded_Dp)) # partial reconstructions for vizualization\n        out_conv = torch.sigmoid(self.decoder_D(decoded_Dr))\n\n        concat = decoded_Dp + decoded_Dr   \n        output_image = torch.sigmoid( self.decoder_D(concat ))\n        return out_phys, hidden1, output_image, out_phys, out_conv        \n\n##############################################################################################################\n\ndevice = torch.device(\"cuda\")\n\nargs = {}\nargs['root'] = '/kaggle/input/mnist_dataset/data'\n\nmm = MovingMNIST(root=args['root'], is_train=True, n_frames_input=10, n_frames_output=10, num_objects=[2])\ntrain_loader = torch.utils.data.DataLoader(dataset=mm, batch_size=64, shuffle=True, num_workers=0)\n\nmm = MovingMNIST(root=args['root'], is_train=False, n_frames_input=10, n_frames_output=10, num_objects=[2])\ntest_loader = torch.utils.data.DataLoader(dataset=mm, batch_size=64, shuffle=False, num_workers=0)\n\nconstraints = torch.zeros((49,7,7)).to(device)\nind = 0\nfor i in range(0,7):\n    for j in range(0,7):\n        constraints[ind,i,j] = 1\n        ind +=1    \n\ndef train_on_batch(input_tensor, target_tensor, encoder, encoder_optimizer, criterion,teacher_forcing_ratio):                \n    encoder_optimizer.zero_grad()\n    # input_tensor : torch.Size([batch_size, input_length, channels, cols, rows])\n    input_length  = input_tensor.size(1)\n    target_length = target_tensor.size(1)\n    loss = 0\n    for ei in range(input_length-1): \n        encoder_output, encoder_hidden, output_image,_,_ = encoder(input_tensor[:,ei,:,:,:], (ei==0) )\n        loss += criterion(output_image,input_tensor[:,ei+1,:,:,:])\n\n    decoder_input = input_tensor[:,-1,:,:,:] # first decoder input = last image of input sequence\n    \n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False \n    for di in range(target_length):\n        decoder_output, decoder_hidden, output_image,_,_ = encoder(decoder_input)\n        target = target_tensor[:,di,:,:,:]\n        loss += criterion(output_image,target)\n        if use_teacher_forcing:\n            decoder_input = target # Teacher forcing    \n        else:\n            decoder_input = output_image\n\n    ## Moment regularization  # encoder.phycell.cell_list[0].F.conv1.weight # size (nb_filters,in_channels,7,7)\n    #k2m = K2M([7,7]).to(device)\n    #for b in range(0,encoder.phycell.cell_list[0].input_dim):\n    #    filters = encoder.phycell.cell_list[0].F.conv1.weight[:,b,:,:] # (nb_filters,7,7)     \n    #    m = k2m(filters.double()) \n    #    m  = m.float()   \n    #    loss += criterion(m, constraints) # constrains is a precomputed matrix   \n    loss.backward()\n    encoder_optimizer.step()\n    return loss.item() / target_length\n\n\ndef trainIters(encoder, nepochs, print_every=10,eval_every=10,name=''):\n    train_losses = []\n    best_mse = float('inf')\n\n    encoder_optimizer = torch.optim.Adam(encoder.parameters(),lr=0.001)\n    scheduler_enc = ReduceLROnPlateau(encoder_optimizer, mode='min', patience=2,factor=0.1,verbose=True)\n    criterion = nn.MSELoss()\n    \n    for epoch in tqdm(range(0, nepochs)):        \n        t0 = time.time()\n        loss_epoch = 0\n        teacher_forcing_ratio = np.maximum(0 , 1 - epoch * 0.003) \n        \n        for i, out in enumerate(train_loader, 0):\n            input_tensor = out[1].to(device)\n            target_tensor = out[2].to(device)\n            loss = train_on_batch(input_tensor, target_tensor, encoder, encoder_optimizer, criterion, teacher_forcing_ratio)                                   \n            loss_epoch += loss\n                      \n        train_losses.append(loss_epoch)        \n        if (epoch+1) % print_every == 0:\n            print('epoch ',epoch,  ' loss ',loss_epoch, ' time epoch ',time.time()-t0)\n            \n        if (epoch+1) % eval_every == 0:\n            mse, mae,ssim = evaluate(encoder,test_loader) \n            scheduler_enc.step(mse)                   \n            torch.save(encoder.state_dict(),'encoder_{}.pth'.format(name))                           \n    return train_losses\n\n    \ndef evaluate(encoder,loader):\n    total_mse, total_mae,total_ssim,total_bce = 0,0,0,0\n    t0 = time.time()\n    with torch.no_grad():\n        for i, out in enumerate(loader, 0):\n            input_tensor = out[1].to(device)\n            target_tensor = out[2].to(device)\n            input_length = input_tensor.size()[1]\n            target_length = target_tensor.size()[1]\n\n            for ei in range(input_length-1):\n                encoder_output, encoder_hidden, _,_,_  = encoder(input_tensor[:,ei,:,:,:], (ei==0))\n\n            decoder_input = input_tensor[:,-1,:,:,:] # first decoder input= last image of input sequence\n            predictions = []\n\n            for di in range(target_length):\n                decoder_output, decoder_hidden, output_image,_,_ = encoder(decoder_input, False, False)\n                decoder_input = output_image\n                predictions.append(output_image.cpu())\n\n            input = input_tensor.cpu().numpy()\n            target = target_tensor.cpu().numpy()\n            predictions =  np.stack(predictions) # (10, batch_size, 1, 64, 64)\n            predictions = predictions.swapaxes(0,1)  # (batch_size,10, 1, 64, 64)\n\n            mse_batch = np.mean((predictions-target)**2 , axis=(0,1,2)).sum()\n            mae_batch = np.mean(np.abs(predictions-target) ,  axis=(0,1,2)).sum() \n            total_mse += mse_batch\n            total_mae += mae_batch\n            for a in range(0,target.shape[0]):\n                for b in range(0,target.shape[1]):\n                    total_ssim += ssim(target[a,b,0,], predictions[a,b,0,], data_range=1.0) / (target.shape[0]*target.shape[1]) \n\n            \n            cross_entropy = -target*np.log(predictions) - (1-target) * np.log(1-predictions)\n            cross_entropy = cross_entropy.sum()\n            cross_entropy = cross_entropy / (64*target_length)\n            total_bce +=  cross_entropy\n     \n    print('eval mse ', total_mse/len(loader),  ' eval mae ', total_mae/len(loader),' eval ssim ',total_ssim/len(loader), ' time= ', time.time()-t0)        \n    return total_mse/len(loader),  total_mae/len(loader), total_ssim/len(loader)\n\n\n# phycell  =  PhyCell(input_shape=(16,16), input_dim=64, F_hidden_dims=[49], n_layers=1, kernel_size=(7,7), device=device) \nconvcell_0 =  ConvLSTM(input_shape=(16,16), input_dim=64, hidden_dims=[128,128,64], n_layers=3, kernel_size=(3,3), device=device)   \nconvcell =  ConvLSTM(input_shape=(16,16), input_dim=64, hidden_dims=[128,128,64], n_layers=3, kernel_size=(3,3), device=device)   \nencoder  = EncoderRNN(convcell_0, convcell, device)\n  \ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n   \nprint('phycell ' , count_parameters(convcell_0) )    \nprint('convcell ' , count_parameters(convcell) ) \nprint('encoder ' , count_parameters(encoder) ) \n\ntrainIters(encoder,200,print_every=1,eval_every=10,name=\"phydnet_double\")\n\n#encoder.load_state_dict(torch.load('save/encoder_phydnet.pth'))\n#encoder.eval()\n#mse, mae,ssim = evaluate(encoder,test_loader) \ntorch.save(encoder.state_dict(),'encoder_{}.pth'.format('phydnet_2_lstm_100'))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:14:56.280616Z","iopub.execute_input":"2024-12-31T15:14:56.280915Z","execution_failed":"2024-12-31T19:26:47.840Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-1-dc691a22a5bb>:758: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"layer  0 input dim  64  hidden dim  128\nlayer  1 input dim  128  hidden dim  128\nlayer  2 input dim  128  hidden dim  64\nlayer  0 input dim  64  hidden dim  128\nlayer  1 input dim  128  hidden dim  128\nlayer  2 input dim  128  hidden dim  64\nphycell  2508032\nconvcell  2508032\nencoder  5368961\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 1/200 [02:36<8:38:49, 156.43s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  0  loss  11.35341339707375  time epoch  156.429993391037\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 2/200 [05:11<8:33:44, 155.68s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  1  loss  5.4381491541862506  time epoch  155.1541042327881\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 3/200 [07:46<8:29:53, 155.30s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  2  loss  4.440904514491558  time epoch  154.8427734375\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 4/200 [10:21<8:26:50, 155.16s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  3  loss  3.60855732858181  time epoch  154.93702459335327\n","output_type":"stream"},{"name":"stderr","text":"  2%|▎         | 5/200 [12:55<8:23:27, 154.91s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  4  loss  4.20542240291834  time epoch  154.46920585632324\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 6/200 [15:30<8:20:31, 154.80s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  5  loss  3.3180296167731282  time epoch  154.59750485420227\n","output_type":"stream"},{"name":"stderr","text":"  4%|▎         | 7/200 [18:04<8:17:22, 154.63s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  6  loss  3.1657181948423396  time epoch  154.25915360450745\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 8/200 [20:39<8:14:31, 154.54s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  7  loss  2.9742852345108988  time epoch  154.34828877449036\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 9/200 [23:13<8:11:42, 154.46s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  8  loss  3.1846848696470254  time epoch  154.29049706459045\nepoch  9  loss  2.6932097733020774  time epoch  154.05561566352844\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 10/200 [27:24<9:43:27, 184.25s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  85.16803712298156  eval mae  164.54955957497762  eval ssim  0.6910831944182351  time=  96.86088585853577\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 11/200 [29:58<9:11:28, 175.07s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  10  loss  3.012785425782206  time epoch  154.2537431716919\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 12/200 [32:33<8:49:00, 168.83s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  11  loss  2.7979340404272075  time epoch  154.57280492782593\n","output_type":"stream"},{"name":"stderr","text":"  6%|▋         | 13/200 [35:07<8:32:58, 164.59s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  12  loss  2.828372929990291  time epoch  154.8161883354187\n","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 14/200 [37:42<8:20:55, 161.59s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  13  loss  2.6637313663959503  time epoch  154.64924144744873\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 15/200 [40:17<8:11:47, 159.50s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  14  loss  2.8799298286438004  time epoch  154.6628212928772\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 16/200 [42:51<8:04:40, 158.05s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  15  loss  2.5241360604763026  time epoch  154.672217130661\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 17/200 [45:26<7:58:57, 157.04s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  16  loss  2.657910476624966  time epoch  154.6861481666565\n","output_type":"stream"},{"name":"stderr","text":"  9%|▉         | 18/200 [48:01<7:54:09, 156.32s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  17  loss  2.4811175420880316  time epoch  154.63875937461853\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 19/200 [50:35<7:50:01, 155.81s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  18  loss  2.544031302630901  time epoch  154.6327784061432\nepoch  19  loss  2.6756771132349977  time epoch  154.56731629371643\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 20/200 [54:46<9:13:06, 184.37s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  101.17265295526784  eval mae  173.46529665418493  eval ssim  0.7705854064136481  time=  96.30354833602905\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 21/200 [57:21<8:43:03, 175.33s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  20  loss  2.8055418491363517  time epoch  154.24146056175232\n","output_type":"stream"},{"name":"stderr","text":" 11%|█         | 22/200 [59:55<8:21:56, 169.19s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  21  loss  2.493461099267005  time epoch  154.88298845291138\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 23/200 [1:02:30<8:06:21, 164.87s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  22  loss  2.4687889292836194  time epoch  154.78537392616272\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 24/200 [1:05:05<7:54:41, 161.83s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  23  loss  2.61089637875557  time epoch  154.73355746269226\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▎        | 25/200 [1:07:40<7:45:47, 159.70s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  24  loss  2.331495662033557  time epoch  154.72553730010986\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 26/200 [1:10:14<7:38:42, 158.17s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  25  loss  2.447394195199012  time epoch  154.6129114627838\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▎        | 27/200 [1:12:49<7:33:05, 157.14s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  26  loss  2.5406135633587845  time epoch  154.7264142036438\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 28/200 [1:15:24<7:28:22, 156.41s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  27  loss  2.5628750711679458  time epoch  154.70391416549683\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 29/200 [1:17:59<7:24:22, 155.92s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  28  loss  2.509886175394059  time epoch  154.7838990688324\nepoch  29  loss  2.2862218528985983  time epoch  154.67048954963684\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 30/200 [1:22:09<8:42:15, 184.33s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  80.67155184411699  eval mae  152.82952948892193  eval ssim  0.8173317472027936  time=  95.88171792030334\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 31/200 [1:24:43<8:13:40, 175.27s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  30  loss  2.4240524813532827  time epoch  154.1378893852234\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 32/200 [1:27:18<7:53:40, 169.17s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  31  loss  2.345496851205826  time epoch  154.93796610832214\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▋        | 33/200 [1:29:53<7:38:41, 164.80s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  32  loss  2.5145298093557353  time epoch  154.59347081184387\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 34/200 [1:32:27<7:27:28, 161.74s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  33  loss  2.441766200959682  time epoch  154.58937454223633\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 35/200 [1:35:02<7:18:51, 159.59s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  34  loss  2.3745816014707106  time epoch  154.57027649879456\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 36/200 [1:37:37<7:12:07, 158.10s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  35  loss  2.281346549093725  time epoch  154.61661052703857\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 37/200 [1:40:11<7:06:44, 157.08s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  36  loss  2.2872685968875888  time epoch  154.71660494804382\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 38/200 [1:42:46<7:02:12, 156.37s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  37  loss  2.45525826588273  time epoch  154.71703839302063\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 39/200 [1:45:21<6:58:10, 155.84s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  38  loss  2.167169623076916  time epoch  154.6063530445099\nepoch  39  loss  2.2605649061501016  time epoch  154.65599298477173\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 40/200 [1:49:32<8:11:53, 184.46s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  60.65981778673306  eval mae  129.2528016886134  eval ssim  0.8508013145331998  time=  96.51109600067139\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 41/200 [1:52:06<7:44:45, 175.38s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  40  loss  2.238712816685438  time epoch  154.2054305076599\n","output_type":"stream"},{"name":"stderr","text":" 21%|██        | 42/200 [1:54:40<7:25:06, 169.03s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  41  loss  2.2352839559316626  time epoch  154.2104630470276\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 43/200 [1:57:15<7:10:45, 164.62s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  42  loss  2.3566626414656633  time epoch  154.3381118774414\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 44/200 [1:59:49<6:59:48, 161.47s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  43  loss  2.203799089789391  time epoch  154.09497928619385\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▎       | 45/200 [2:02:23<6:51:36, 159.33s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  44  loss  2.355782863497734  time epoch  154.35280466079712\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 46/200 [2:04:57<6:45:06, 157.84s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  45  loss  2.3212297864258287  time epoch  154.34754085540771\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▎       | 47/200 [2:07:32<6:39:44, 156.76s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  46  loss  2.342099750787018  time epoch  154.24824047088623\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 48/200 [2:10:06<6:35:15, 156.03s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  47  loss  2.301534481346609  time epoch  154.31097888946533\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 49/200 [2:12:41<6:31:40, 155.64s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  48  loss  2.0586811490356927  time epoch  154.7213954925537\nepoch  49  loss  2.2463012814521797  time epoch  154.6271686553955\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 50/200 [2:16:51<7:40:11, 184.08s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  62.656764911238554  eval mae  140.0034155390065  eval ssim  0.8407803741700053  time=  95.76872324943542\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 51/200 [2:19:26<7:15:12, 175.25s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  50  loss  2.325495649874211  time epoch  154.65447187423706\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 52/200 [2:22:00<6:56:48, 168.98s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  51  loss  2.1008713394403467  time epoch  154.3311357498169\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▋       | 53/200 [2:24:35<6:43:28, 164.69s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  52  loss  2.0709684275090696  time epoch  154.67521214485168\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 54/200 [2:27:10<6:33:30, 161.71s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  53  loss  2.2054415859282024  time epoch  154.7761673927307\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 55/200 [2:29:44<6:25:48, 159.64s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  54  loss  2.2528974950313563  time epoch  154.81499767303467\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 56/200 [2:32:19<6:19:42, 158.21s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  55  loss  2.2991992667317396  time epoch  154.85941433906555\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 57/200 [2:34:54<6:14:38, 157.19s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  56  loss  2.151842462271451  time epoch  154.80725646018982\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▉       | 58/200 [2:37:29<6:10:05, 156.38s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  57  loss  2.1164078205823897  time epoch  154.47747993469238\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 59/200 [2:40:03<6:06:06, 155.79s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  58  loss  2.1091720901429656  time epoch  154.43157482147217\nepoch  59  loss  2.1311471253633503  time epoch  154.37821650505066\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 60/200 [2:44:12<7:08:57, 183.84s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  60.14990171201669  eval mae  138.33951349926602  eval ssim  0.835529961465205  time=  94.8445360660553\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 61/200 [2:46:47<6:45:22, 174.98s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  60  loss  2.152662718296051  time epoch  154.3217248916626\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 62/200 [2:49:21<6:28:13, 168.79s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  61  loss  2.226773326098918  time epoch  154.3374683856964\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 63/200 [2:51:55<6:15:33, 164.48s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  62  loss  2.1060519151389596  time epoch  154.40716218948364\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 64/200 [2:54:30<6:05:56, 161.44s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  63  loss  2.178011911362411  time epoch  154.36801385879517\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▎      | 65/200 [2:57:05<5:58:54, 159.52s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  64  loss  2.080204079300166  time epoch  155.0159204006195\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 66/200 [2:59:40<5:53:25, 158.25s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  65  loss  2.0172631569206714  time epoch  155.28581523895264\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▎      | 67/200 [3:02:15<5:48:45, 157.34s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  66  loss  2.18409513682127  time epoch  155.2074112892151\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 68/200 [3:04:50<5:44:24, 156.55s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  67  loss  2.1557837381958955  time epoch  154.72152495384216\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 69/200 [3:07:25<5:40:31, 155.97s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  68  loss  2.153947724401951  time epoch  154.60877466201782\nepoch  69  loss  2.069047236442564  time epoch  154.75003480911255\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 70/200 [3:11:36<6:39:59, 184.61s/it]","output_type":"stream"},{"name":"stdout","text":"eval mse  47.16546949155771  eval mae  116.25654538877451  eval ssim  0.8592204888843237  time=  96.64838004112244\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 71/200 [3:14:10<6:17:11, 175.44s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  70  loss  1.9839581549167635  time epoch  154.03960728645325\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 72/200 [3:16:44<6:00:28, 168.98s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  71  loss  2.242801770567894  time epoch  153.88869714736938\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▋      | 73/200 [3:19:18<5:48:05, 164.45s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  72  loss  2.045995590090752  time epoch  153.9022340774536\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 74/200 [3:21:52<5:38:42, 161.29s/it]","output_type":"stream"},{"name":"stdout","text":"epoch  73  loss  2.1576069436967384  time epoch  153.89982914924622\n","output_type":"stream"}],"execution_count":null}]}